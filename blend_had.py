import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import preprocess_input
from keras.applications.vgg19 import decode_predictions
from keras.applications.vgg19 import VGG19
from keras.models import Sequential
from keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D,Dropout,Activation,BatchNormalization,  GlobalAveragePooling2D
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras import optimizers
from tensorflow.keras import regularizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.model_selection import KFold
from tensorflow.keras.callbacks import LearningRateScheduler
# from keras.optimizers import Adam
from imblearn.over_sampling import SMOTE
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import RandomOverSampler
from collections import Counter
from sklearn.metrics import accuracy_score, precision_score
from sklearn.metrics import f1_score
from scipy.linalg import hadamard
import cv2
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.models import Model
from tensorflow.keras import applications
from keras.utils import np_utils
from scipy.spatial import distance
from tensorflow.keras import backend
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True  # Allow GPU memory growth
session = tf.compat.v1.Session(config=config)
tf.compat.v1.keras.backend.set_session(session)
dropout = 0.25

seed_value = 40
tf.random.set_seed(seed_value)
np.random.seed(seed_value)
import os
# os.environ["TF_GPU_ALLOCATOR"] = "cuda_malloc_async"

# gpu_available = tf.config.list_physical_devices('GPU')
#
# if gpu_available:
#     print("GPU is available and active.")
# else:
#     print("No GPU available.")

H = hadamard(64)
H[H == -1] = 0
H = H[1:41]
# print(H)

dropout = 0.25
def hadamard_encode(x, H):
    encoded = np.zeros((len(x), 64))
    for idx, val in enumerate(x):
        # for item in val:
        #     curr = int(item)
        encoded[idx] = H[val]
    return encoded


def scheduler(epoch):
    if epoch < 5:
        return 0.04
    if epoch < 10:
        return 0.015
    return 0.01

def collect_data(parent_dir, list_groups ):
    data = []
    labels = []
    for group_id, group_name in enumerate(list_groups):
        label = group_id
        dir_path = f'{parent_dir}/{group_name}'
        # print(f"{group_name}")
        # print(label)
        files = [(filename, f'{dir_path}/{filename}')
                    for filename in os.listdir(dir_path)
                        if os.path.isfile(f'{dir_path}/{filename}')]
        for filename, filepath in files:
            labels.append(label)
            img = tf.io.read_file(filepath)
            img = tf.image.decode_png(img, channels = 3)
            img = tf.image.resize(img,(128, 128))
            img = (img - np.min(img)) / (np.max(img) - np.min(img))

            data.append(img)
    return data, labels

# def load_data(dataset, standardize=True):
#     """
#     :param dataset:
#     :param standardize:
#     :return:
#     """
#
#     features = dataset['arr'][:, 0]
#     features = np.array([feature for feature in features])
#     features = np.reshape(
#         features, (features.shape[0], features.shape[1] * features.shape[2]))
#     features = features / 255.0
#     print(np.max(features))
#     # if standardize:
#     #     features = StandardScaler().fit_transform(features)
#
#     labels = dataset['arr'][:, 1]
#     labels = np.array([label for label in labels])
#
#     return features, labels

def oversample(X_train, y_train):
    num_samples = X_train.shape[0]
    num_features = np.prod(X_train.shape[1:])
    X_train_reshaped = X_train.reshape((num_samples, num_features))
    smote = SMOTE(random_state =42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_reshaped, y_train)
    # Check class distribution after oversampling
    X_train_resampled_4d = X_train_resampled.reshape((-1, ) + X_train.shape[1:])
    # print("Class distribution after oversampling:", Counter(y_train_resampled))
    return X_train_resampled_4d, y_train_resampled

# def oversample(X_train, y_train):
#     # y_train = y_train.tolist()
#     #
#     # y_train = [label[0] for label in y_train]
#     # y_train = np.array(y_train)
#     # Define the classes you want to oversample (replace [0, 1] with your desired class labels)
#     classes_to_oversample = [3, 8, 9, 19, 23, 24, 32]
#
#     # Filter feature vectors and labels for the classes to oversample
#     X_train_to_oversample = X_train[np.isin(y_train, classes_to_oversample)]
#     y_train_to_oversample = y_train[np.isin(y_train, classes_to_oversample)]
#     # print(X_train_to_oversample)
#     # Instantiate RandomOverSampler
#     oversampler = RandomOverSampler(random_state=42)
#     n_samples, height, width, channels = X_train_to_oversample.shape
#     X_train_to_oversample_reshaped = X_train_to_oversample.reshape(n_samples, height * width * channels)
#     # Reshape X_train_to_oversample if needed
#     X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_to_oversample_reshaped, y_train_to_oversample)
#     n_samples_resampled, _ = X_train_resampled.shape
#     X_train_resampled_original_shape = X_train_resampled.reshape(n_samples_resampled, height, width, channels)
#     # # Combine oversampled data with original data
#     X_train_resampled = np.concatenate([X_train, X_train_resampled_original_shape])
#     y_train_resampled = np.concatenate([y_train, y_train_resampled])
#
#     # Check class distribution after oversampling
#     print("Class distribution after oversampling:", Counter(y_train_resampled))
#     return X_train_resampled, y_train_resampled


# Define your augmentation function
def augment_image(image, label):
    # print(image.shape)
    image = tf.image.resize(image, [128, 128])
    # print(image.shape)
    # Randomly flip the image horizontally
    # image = tf.image.random_flip_left_right(image)
    #
    # # Randomly flip the image vertically
    # image = tf.image.random_flip_up_down(image)
    #
    # # Randomly rotate the image
    # image = tf.image.rot90(image, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))
    #
    # # Randomly adjust brightness of the image
    # image = tf.image.random_brightness(image, max_delta=0.2)
    #
    # # Randomly adjust contrast of the image
    # image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
    # # print("images", image.shape)
    return image, label



def build_model():
    vgg = Sequential()
    vgg.add(Conv2D(3, (3, 3), padding='same',input_shape=(128,128,3)))
    vgg.add(Activation('relu'))

    _vgg = VGG16(weights='imagenet', include_top=False)
#or
    # _vgg = VGG19(weights='imagenet', include_top=False)

    counter=0
    for layer in _vgg.layers:
        layer.trainable = False
        counter+=1
    # print("VGG's ", counter , " layers are not added to the layer")
    vgg.add(_vgg)
    # print("done")

    vgg.add(Flatten())

    vgg.add(Dense(512,activation='relu'))
    vgg.add(Dense(64,activation="softmax"))
    return vgg
# vgg.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
batch_size = 20
epochs  = 80

# def preprocess(data, labels):
#     # print("new labels.shape  = ", labels.shape)
#     labels = labels.reshape(-1, 1)
#     # print("data.shape before  = ", data.shape)
#     data = data.reshape((len(data), 32, 32, 1))
#     # print("data.shape after  = ", data.shape)
#     # print("labels.shape  = ", labels.shape)
#     data = data.astype('float32')
#     labels = labels.astype('float32')
#
#     return data, labels

def transform(y_train, y_test):
    # enc = OneHotEncoder()
    # enc.fit(y_train)
    #
    # y_train = enc.transform(y_train).toarray()
    # y_test = enc.transform(y_test).toarray()
    # print("y_train.shape  = ", y_train.shape)
    # print("y_test.shape  = ", y_test.shape)
    y_train = hadamard_encode(y_train, H)
    y_test = hadamard_encode(y_test, H)
    return y_train, y_test

def new_accuracy(y_true, y_pred):
    y_pred_thre = backend.cast(backend.greater(backend.clip(y_pred, 0, 1), 0.5), dtype = 'float32')
    res = backend.mean(backend.cast(backend.equal(y_pred_thre, y_true), dtype = 'float32'), axis=1)
    accuracy = backend.mean(backend.cast(backend.equal(res, 1), dtype = 'float32'))
    return accuracy

def hamming_dis(y_pred, H):
    dis = distance.cdist(y_pred, H, 'euclidean')
    index = np.argmin(dis, axis=1)
    true = []
    for i in index:
        true.append(H[i])
    # print(true[12:14])
    return true

def validate_accuracy(y_true, y_pred):
    # y_pred_thre = tf.cast(tf.greater(tf.clip_by_value(y_pred, 0, 1), 0.5), dtype='float32')
    # print((np.greater(np.clip(y_pred, 0, 1), 0.5).astype(float))[12:14])
    y_predm = tf.convert_to_tensor(hamming_dis(y_pred, H))
    res = tf.reduce_mean(tf.cast(tf.equal(y_predm, y_true), dtype = 'float32'), axis=1)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(res, 1), dtype = 'float32'))
    return accuracy

if __name__ == '__main__':
    PATH = "C:/gitproject/malware/Malware/archive/malware_dataset"
    os.listdir(PATH)
    dirs = [f'{PATH}/{d}/' for d in os.listdir(PATH) if os.path.isdir(f'{PATH}/{d}')]
    train_dir, test_dir = (
        dirs[0], dirs[1]) if 'train' in dirs[0] else (dirs[1], dirs[0])
    list_groups = [group for group in os.listdir(train_dir)]
    # print(os.listdir(train_dir))
    # print(list_groups)
    X_train, y_train = collect_data(parent_dir=train_dir, list_groups=list_groups)
    X_test, y_test = collect_data(parent_dir=test_dir, list_groups=list_groups)
    y_train = hadamard_encode(y_train, H)
    y_test = hadamard_encode(y_test, H)


    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2)
    X_train = np.array(X_train)
    X_test = np.array(X_test)
    y_train = np.array(y_train)
    y_test = np.array(y_test)
    # X_val = np.array(X_val).astype('float32')
    # y_train = np_utils.to_categorical(np.array(y_train))
    # y_test = np_utils.to_categorical(np.array(y_test))
    # y_val = np_utils.to_categorical(np.array(y_val))
    # smote = SMOTE(random_state=42)
    # X_resampled_train, y_resampled_train = smote.fit_resample(X_train, y_train)
    # print("size", len(y_resampled_train))
    print(X_train.shape)
    print(X_test.shape)
    # y_train_list = y_train.tolist()
    # # # print(y_train_list)
    # y_train_list = [label[0] for label in y_train]
    # unique_labels, counts = np.unique(y_train, return_counts = True)
    #
    # for label, count in zip(unique_labels, counts):
    #     print(label, count)
    # # print(y_train_list)
    # class_labels = np.unique(y_train_list)
    # class_weights = compute_class_weight('balanced', np.unique(y_train_list), y_train_list)
    #
    # # Convert class labels to integers (if they are not already)
    # class_labels_int = list(map(int, class_labels))
    #
    # # Create a dictionary mapping class labels (ints) to class weights
    # class_weight_dict = dict(zip(class_labels_int, class_weights))
    # print(class_weight_dict)

    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=0.01,
        decay_steps=10000,
        decay_rate=0.9)
    sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
    filepath = "blendedhad.hdf5"  ##lecif10 use for weights disturbance
    checkpoint = ModelCheckpoint(filepath, monitor='val_new_accuracy', verbose=1, save_best_only=True,
                                 mode='max')

    num_splits = 5
    smote = SMOTE(random_state=42)
    # Initialize KFold cross-validation
    kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)

    # Initialize a list to store the accuracy scores
    fold_accuracy = []
    fold_loss = []
    accuracy_pre = []

    # Loop through each fold
    for train_index, val_index in kf.split(X_train):
        # Split the data into training and testing sets for this fold
        # print(train_index, val_index)
        X_train_fold, X_test_fold = X_train[train_index], X_train[val_index]
        y_train_fold, y_test_fold = y_train[train_index], y_train[val_index]
        # X_train_fold, y_train_fold = oversample(X_train_fold, y_train_fold)


        # X_train_fold, y_train_fold = preprocess(X_train_fold, y_train_fold)
        # X_test_fold, y_test_fold = preprocess(X_test_fold, y_test_fold)
        # y_train_fold = np.array(y_train_fold).reshape(-1, 1)
        # y_test_fold = np.array(y_test_fold).reshape(-1, 1)
        # y_train_fold, y_test_fold = transform(y_train_fold, y_test_fold)
        print(y_train_fold.shape, y_test_fold.shape)
        # X_test_fold = tf.image.resize(X_test_fold, [64, 64]).numpy()

        train_dataset = tf.data.Dataset.from_tensor_slices((X_train_fold, y_train_fold))

        # augmented_train_dataset = train_dataset.map(augment_image)
        augmented_train_dataset = train_dataset.shuffle(buffer_size=len(X_train_fold)).batch(batch_size)


        # print("fold", X_test_fold.shape, y_test_fold.shape)
        # print("fold", X_train_fold.shape, y_train_fold.shape)


        vgg = build_model()
        vgg.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=[new_accuracy])

        early = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')
        cbks = [checkpoint, early]
        # history = vgg.fit(augmented_train_dataset.repeat(),
        #         # class_weight=class_weight_dict,
        #         steps_per_epoch=len(X_train_fold) // batch_size,
        #         epochs=180,
        #         callbacks = cbks,
        #         verbose = 1)
        vgg.fit(augmented_train_dataset.repeat(),
                # class_weight=class_weight_dict,
                steps_per_epoch=len(X_train_fold) // batch_size,
                epochs=2,
                callbacks=cbks,
                validation_data=(X_test_fold, y_test_fold))

        PREDICTED_CLASSES = vgg.predict(X_test_fold, verbose=0)
        print(PREDICTED_CLASSES.shape)
        print(y_test_fold.shape)
        accuracy = validate_accuracy(y_test_fold, PREDICTED_CLASSES)
        print(accuracy)
        # accuracy_scores.append(acc)
        # loss, accuracy = validate_accuracy(X_test_fold, y_test_fold, verbose=0)
        # fold_loss.append(loss)
        fold_accuracy.append(accuracy)

    y_test = np.array(y_test).reshape(-1, 1)
    PREDICTED_CLASSES = vgg.predict(X_test, verbose=0)
    # temp = sum( y_test== np.argmax(PREDICTED_CLASSES, axis=1).reshape(-1, 1))
    acc = validate_accuracy(y_test, PREDICTED_CLASSES)
    # print(acc)
    # Append scores to lists
    # accuracy_pre.append(acc)
    # average_accuracy = np.mean(accuracy_pre)


    print("prediction Accuracy:", acc)

    # print(f'Average loss: {np.mean(fold_loss)}')
    print(f'Average accuracy: {np.mean(fold_accuracy)}')
    # vgg.save('model.h5')


