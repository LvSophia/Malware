import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from keras.applications.vgg16 import VGG16
from keras.models import Sequential
from keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D,Dropout,Activation,BatchNormalization
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.callbacks import LearningRateScheduler
from keras.optimizers import Adam
dropout = 0.25

seed_value = 40
tf.random.set_seed(seed_value)
np.random.seed(seed_value)

def scheduler(epoch):
    if epoch < 5:
        return 0.04
    if epoch < 10:
        return 0.015
    return 0.01

def load_data(dataset, standardize=True):
    """
    :param dataset:
    :param standardize:
    :return:
    """

    features = dataset['arr'][:, 0]
    features = np.array([feature for feature in features])
    features = np.reshape(
        features, (features.shape[0], features.shape[1] * features.shape[2]))

    if standardize:
        features = StandardScaler().fit_transform(features)

    labels = dataset['arr'][:, 1]
    labels = np.array([label for label in labels])

    return features, labels

def build_model():
    model = Sequential()
    model.add((Conv2D(32, (3, 3), padding='valid', activation='relu', kernel_initializer='he_normal',
                       input_shape=(32, 32, 1))))
    # model.add(GaussianNoise(0.5))
    # model.add(quantize_annotate_layer(Activation('relu')))
    # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(64, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(64, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(128, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(128, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    model.add(BatchNormalization())

    model.add(Flatten())
    model.add((Dense(1024, activation='relu', kernel_initializer='he_normal')))  # relu
    # model.add(Dropout(dropout))
    model.add((Dense(512, activation='relu', kernel_initializer='he_normal')))  # relu
    model.add((Dense(25, activation='softmax', kernel_initializer='he_normal')))  # softmax

    return model

# def build_model():
#     vgg = Sequential()
#     vgg.add(Conv2D(3, (3, 3), padding='same',input_shape=(32,32,1)))
#     vgg.add(Activation('relu'))
#
#     _vgg = VGG16(weights='imagenet', include_top=False)
# #or
# # _vgg = VGG19(weights='imagenet', include_top=False)
#
#     counter=0
#     for layer in _vgg.layers:
#         layer.trainable = False
#         counter+=1
#     print("VGG's ", counter , " layers are not added to the layer")
#     vgg.add(_vgg)
#     print("done")
#
#     vgg.add(Flatten())
#
#     vgg.add(Dense(128,activation='relu'))
#     vgg.add(Dense(25,activation="softmax"))
#     return vgg
# vgg.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
batch_size = 20
epochs  = 80


if __name__ == '__main__':
    data, labels = load_data(np.load('malimg.npz', allow_pickle=True))
    labels = labels.reshape((9339, 1))
    print("data.shape  = ", data.shape)
    print("labels.shape  = ", labels.shape)

    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.101)
    # print(y_test)
    enc = OneHotEncoder()
    enc.fit(y_train)
    print(y_train.shape)
    y_train = enc.transform(y_train).toarray()
    y_test = enc.transform(y_test).toarray()
    # print(y_test[0:3][0:24])
    # rehsaping traing
    print("X_train.shape before  = ", X_train.shape)
    X_train = X_train.reshape((len(X_train), 32, 32, 1))
    print("X_train.shape after  = ", X_train.shape)
    print("y_train.shape  = ", y_train.shape)

    # rehsaping testing
    print("X_test.shape before  = ", X_test.shape)
    X_test = X_test.reshape((len(X_test ), 32, 32, 1))
    print("X_test.shape after  = ", X_test.shape)
    print("y_test.shape  = ", y_test.shape)
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)

    filepath = "mal.hdf5"  ##lecif10 use for weights disturbance
    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True,
                                 mode='max')
    early = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')
    vgg = build_model()
    # opt = Adam(lr=0.001)
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=1e-2,
        decay_steps=10000,
        decay_rate=0.9)
    sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
    vgg.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])
    # vgg.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    # change_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)
    cbks = [checkpoint, early]

    datagen = ImageDataGenerator(horizontal_flip=True,
                                 width_shift_range=0.125, height_shift_range=0.125, fill_mode='constant', cval=0.)
    # datagen.fit(X_train)
    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)
    num_batches = len(list(dataset))
    vgg.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                steps_per_epoch=391,
                epochs=180,
                callbacks = cbks,
                validation_data = (X_test, y_test))
    # vgg.save('model.h5')


    # vari=vgg.predict(X_test.reshape((X_test.shape[0],32,32,1)))
    # vari2=np.argmax(vari,axis=1)
    # print("starting evaluating")
    # # vgg.evaluate(X_test, y_test, verbose=0)
    # print("done dude")
    accu = []
    for i in range(100):
        model3 = build_model()
        model3.load_weights('mal.hdf5')

        PREDICTED_CLASSES = model3.predict(X_test, verbose=0)
        # acc = validate_accuracy(y_test, PREDICTED_CLASSES)
        temp = sum(np.argmax(y_test, axis=1) == np.argmax(PREDICTED_CLASSES, axis=1))
        acc = temp / len(y_test)
        accu.append(acc)
    accnew = np.mean(accu)
    print("\nTest accuracy: %.1f%%" % (100.0 * accnew))