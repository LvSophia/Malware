import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from keras.applications.vgg16 import VGG16
from keras.models import Sequential
from keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D,Dropout,Activation,BatchNormalization,  GlobalAveragePooling2D
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras import optimizers
from tensorflow.keras import regularizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.model_selection import KFold
from tensorflow.keras.callbacks import LearningRateScheduler
# from keras.optimizers import Adam
from imblearn.over_sampling import SMOTE
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import RandomOverSampler
from collections import Counter
from sklearn.metrics import accuracy_score, precision_score
from sklearn.metrics import f1_score
import cv2
from tensorflow.keras import applications
from keras.utils import np_utils
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True  # Allow GPU memory growth
session = tf.compat.v1.Session(config=config)
tf.compat.v1.keras.backend.set_session(session)
dropout = 0.25

seed_value = 40
tf.random.set_seed(seed_value)
np.random.seed(seed_value)
import os
# os.environ["TF_GPU_ALLOCATOR"] = "cuda_malloc_async"

def scheduler(epoch):
    if epoch < 5:
        return 0.04
    if epoch < 10:
        return 0.015
    return 0.01

def collect_data(parent_dir, list_groups ):
    data = []
    labels = []
    for group_id, group_name in enumerate(list_groups):
        label = group_id
        dir_path = f'{parent_dir}/{group_name}'
        # print(f"{group_name}")
        # print(label)
        files = [(filename, f'{dir_path}/{filename}')
                    for filename in os.listdir(dir_path)
                        if os.path.isfile(f'{dir_path}/{filename}')]
        for filename, filepath in files:
            labels.append(label)
            img = cv2.imread(filepath)

            img = tf.image.resize(img,(128, 128))
            img = (img - np.min(img)) / (np.max(img) - np.min(img))

            data.append(img)
    return data, labels

# def load_data(dataset, standardize=True):
#     """
#     :param dataset:
#     :param standardize:
#     :return:
#     """
#
#     features = dataset['arr'][:, 0]
#     features = np.array([feature for feature in features])
#     features = np.reshape(
#         features, (features.shape[0], features.shape[1] * features.shape[2]))
#     features = features / 255.0
#     print(np.max(features))
#     # if standardize:
#     #     features = StandardScaler().fit_transform(features)
#
#     labels = dataset['arr'][:, 1]
#     labels = np.array([label for label in labels])
#
#     return features, labels

def oversample(X_train, y_train):
    # y_train = y_train.tolist()
    #
    # y_train = [label[0] for label in y_train]
    y_train = np.array(y_train)
    # Define the classes you want to oversample (replace [0, 1] with your desired class labels)
    classes_to_oversample = [0, 1, 4, 5, 6, 7, 8]

    # Filter feature vectors and labels for the classes to oversample
    X_train_to_oversample = X_train[np.isin(y_train, classes_to_oversample)]
    y_train_to_oversample = y_train[np.isin(y_train, classes_to_oversample)]
    # print(X_train_to_oversample)
    # Instantiate RandomOverSampler
    oversampler = RandomOverSampler(random_state=42)
    n_samples, height, width, channels = X_train_to_oversample.shape
    X_train_to_oversample_reshaped = X_train_to_oversample.reshape(n_samples, height * width * channels)
    # Reshape X_train_to_oversample if needed
    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_to_oversample_reshaped, y_train_to_oversample)
    n_samples_resampled, _ = X_train_resampled.shape
    X_train_resampled_original_shape = X_train_resampled.reshape(n_samples_resampled, height, width, channels)
    # # Combine oversampled data with original data
    X_train_resampled = np.concatenate([X_train, X_train_resampled_original_shape])
    y_train_resampled = np.concatenate([y_train, y_train_resampled])

    # Check class distribution after oversampling
    # print("Class distribution after oversampling:", Counter(y_train_resampled))
    return X_train_resampled, y_train_resampled

# Define your augmentation function
def augment_image(image, label):
    # print(image.shape)
    image = tf.image.resize(image, [128, 128])
    # print(image.shape)
    # Randomly flip the image horizontally
    # image = tf.image.random_flip_left_right(image)
    #
    # # Randomly flip the image vertically
    # image = tf.image.random_flip_up_down(image)
    #
    # # Randomly rotate the image
    # image = tf.image.rot90(image, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))
    #
    # # Randomly adjust brightness of the image
    # image = tf.image.random_brightness(image, max_delta=0.2)
    #
    # # Randomly adjust contrast of the image
    # image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
    # # print("images", image.shape)
    return image, label


# def build_model():
#     model = Sequential()
#
#     # First convolutional layer
#     model.add(
#         Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=(128,128,3), padding='same'))
#     model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
#
#     # Second convolutional layer
#     model.add(Conv2D(256, kernel_size=(5, 5), activation='relu', padding='same'))
#     model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
#
#     # Third convolutional layer
#     model.add(Conv2D(384, kernel_size=(3, 3), activation='relu', padding='same'))
#
#     # Fourth convolutional layer
#     model.add(Conv2D(384, kernel_size=(3, 3), activation='relu', padding='same'))
#
#     # Fifth convolutional layer
#     model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))
#     model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
#
#     # Flatten the output and add fully connected layers
#     model.add(Flatten())
#     model.add(Dense(4096, activation='relu'))
#     model.add(Dropout(0.5))
#     model.add(Dense(4096, activation='relu'))
#     model.add(Dropout(0.5))
#     model.add(Dense(9, activation='softmax'))
#
#     return model
# def build_model():
#     model = Sequential()
#     model.add((Conv2D(32, (3, 3), padding='valid', activation='relu', kernel_initializer='he_normal',
#                       kernel_regularizer=regularizers.l2(0.002), input_shape=(128, 128, 3))))
#     # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
#     model.add(BatchNormalization())
#
#     model.add((Conv2D(32, (3, 3), padding='valid', activation='relu',
#                       kernel_initializer='he_normal')))
#     model.add(MaxPooling2D((2, 2), strides=(2, 2)))
#     model.add(BatchNormalization())
#
#     model.add((Conv2D(64, (3, 3), padding='valid', activation='relu',
#                       kernel_initializer='he_normal')))
#     # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
#     model.add(BatchNormalization())
#
#     model.add((Conv2D(64, (3, 3), padding='valid', activation='relu',
#                       kernel_initializer='he_normal')))
#     # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
#     model.add(BatchNormalization())
#
#     model.add((Conv2D(128, (3, 3), padding='valid', activation='relu',
#                       kernel_initializer='he_normal')))
#     model.add(MaxPooling2D((2, 2), strides=(2, 2)))
#     model.add(BatchNormalization())
#
#     model.add((Conv2D(128, (3, 3), padding='valid', activation='relu',
#                       kernel_initializer='he_normal')))
#     model.add(BatchNormalization())
#
#     model.add((Conv2D(256, (3, 3), padding='valid', activation='relu',
#                       kernel_initializer='he_normal')))
#     model.add(BatchNormalization())
#
#     model.add((Conv2D(256, (3, 3), padding='valid', activation='relu',
#                       kernel_initializer='he_normal')))
#     model.add(BatchNormalization())
#
#     model.add(Flatten())
#     model.add((Dense(1024, activation='relu', kernel_initializer='he_normal')))  # relu
#     # model.add(Dropout(dropout))
#     model.add((Dense(512, activation='relu', kernel_initializer='he_normal')))  # relu
#     model.add((Dense(9, activation='softmax', kernel_initializer='he_normal')))  # softmax
#
#     return model
def build_model():
    resnet50_model = applications.ResNet50V2(input_shape=(128,128,3), include_top = False, weights = "imagenet")

    for layer in resnet50_model.layers:
        layer.trainable = False
    add_model = Sequential()
    add_model.add(resnet50_model)
    add_model.add(GlobalAveragePooling2D())
    add_model.add(Dropout(0.5))
    add_model.add(Dense(9,
                    activation='softmax'))
    return add_model
# def build_model():
#     vgg = Sequential()
#     vgg.add(Conv2D(3, (3, 3), padding='same',input_shape=(128,128,3)))
#     vgg.add(Activation('relu'))
#
#     _vgg = VGG16(weights='imagenet', include_top=False)
# #or
# # _vgg = VGG19(weights='imagenet', include_top=False)
#
#     counter=0
#     for layer in _vgg.layers:
#         layer.trainable = False
#         counter+=1
#     print("VGG's ", counter , " layers are not added to the layer")
#     vgg.add(_vgg)
#     print("done")
#
#     vgg.add(Flatten())
#
#     vgg.add(Dense(128,activation='relu'))
#     vgg.add(Dense(9,activation="softmax"))
#     return vgg
# vgg.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
batch_size = 20
epochs  = 80

# def preprocess(data, labels):
#     # print("new labels.shape  = ", labels.shape)
#     labels = labels.reshape(-1, 1)
#     # print("data.shape before  = ", data.shape)
#     data = data.reshape((len(data), 32, 32, 1))
#     # print("data.shape after  = ", data.shape)
#     # print("labels.shape  = ", labels.shape)
#     data = data.astype('float32')
#     labels = labels.astype('float32')
#
#     return data, labels

def transform(y_train, y_test):
    enc = OneHotEncoder()
    enc.fit(y_train)

    y_train = enc.transform(y_train).toarray()
    y_test = enc.transform(y_test).toarray()
    # print("y_train.shape  = ", y_train.shape)
    # print("y_test.shape  = ", y_test.shape)
    return y_train, y_test

if __name__ == '__main__':
    PATH = "C:/gitproject/malware/Malware/big2015_g"
    os.listdir(PATH)
    dirs = [f'{PATH}/{d}/' for d in os.listdir(PATH) if os.path.isdir(f'{PATH}/{d}')]
    train_dir, test_dir = (
        dirs[0], dirs[1]) if 'train' in dirs[0] else (dirs[1], dirs[0])
    list_groups = [group for group in os.listdir(train_dir)]

    print(list_groups)
    X_train, y_train = collect_data(parent_dir=train_dir, list_groups=list_groups)
    X_test, y_test = collect_data(parent_dir=test_dir, list_groups=list_groups)



    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2)
    X_train = np.array(X_train).astype('float32')
    X_test = np.array(X_test).astype('float32')
    y_train = np.array(y_train)
    y_test = np.array(y_test)
    # X_val = np.array(X_val).astype('float32')
    # y_train = np_utils.to_categorical(np.array(y_train))
    # y_test = np_utils.to_categorical(np.array(y_test))
    # y_val = np_utils.to_categorical(np.array(y_val))
    # smote = SMOTE(random_state=42)
    # X_resampled_train, y_resampled_train = smote.fit_resample(X_train, y_train)
    # print("size", len(y_resampled_train))
    # print(len(y_train))
    print(X_test.shape)
    # y_train_list = y_train.tolist()
    # # # print(y_train_list)
    # y_train_list = [label[0] for label in y_train]
    # unique_labels, counts = np.unique(y_train, return_counts = True)
    #
    # for label, count in zip(unique_labels, counts):
    #     print(label, count)
    # # print(y_train_list)
    # class_labels = np.unique(y_train_list)
    # class_weights = compute_class_weight('balanced', np.unique(y_train_list), y_train_list)
    #
    # # Convert class labels to integers (if they are not already)
    # class_labels_int = list(map(int, class_labels))
    #
    # # Create a dictionary mapping class labels (ints) to class weights
    # class_weight_dict = dict(zip(class_labels_int, class_weights))
    # print(class_weight_dict)

    # print(y_test[0:3][0:24])
    # rehsaping traing
    # y_train = y_train.reshape(-1, 1)
    # print("X_train.shape before  = ", X_train.shape)
    # X_train = X_train.reshape((len(X_train), 32, 32, 1))
    # print("X_train.shape after  = ", X_train.shape)
    # print("y_train.shape  = ", y_train.shape)
    #
    # # rehsaping testing
    # print("X_test.shape before  = ", X_test.shape)
    # X_test = X_test.reshape((len(X_test ), 32, 32, 1))
    # print("X_test.shape after  = ", X_test.shape)
    # print("y_test.shape  = ", y_test.shape)
    # X_train = X_train.astype('float32')
    # X_test = X_test.astype('float32')
    # X_test = tf.image.resize(X_test, [64, 64]).numpy()
    # enc = OneHotEncoder()
    # enc.fit(y_train)
    #
    # y_train = enc.transform(y_train).toarray()
    # y_test = enc.transform(y_test).toarray()
    # print(y_train.shape)
    # print("y_test.shape  = ", y_test.shape)




    # augmented_train_dataset = augmented_train_dataset.shuffle(buffer_size=len(X_train)).batch(batch_size)

    # Initialize a counter variable
    # num_samples = 0
    #
    # # Iterate through the augmented dataset to count samples
    # for _ in augmented_train_dataset:
    #     num_samples += 1
    #
    # # Print the number of samples
    # print("Number of samples in x_train after data augmentation:", num_samples)
    # label_counts = {}
    #
    # # Iterate through the dataset to count label occurrences
    # for _, label in augmented_train_dataset:
    #     label = label.numpy()  # Convert label tensor to numpy array
    #
    #     label_tuple = tuple(label.tolist())  # Convert label numpy array to tuple
    #     if label_tuple in label_counts:
    #         label_counts[label_tuple] += 1
    #     else:
    #         label_counts[label_tuple] = 1
    #
    # # Print label counts
    # for label, count in label_counts.items():
    #     print(f"Label {label}: Count {count}")


    filepath = "microsoft.hdf5"  ##lecif10 use for weights disturbance
    checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True,
                                 mode='max')


    # opt = Adam(lr=0.001)
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=1e-3,
        decay_steps=10000,
        decay_rate=0.9)
    sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule)

    # vgg.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    # change_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)

    num_splits = 5

    # Initialize KFold cross-validation
    kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)

    # Initialize a list to store the accuracy scores
    fold_accuracy = []
    fold_loss = []
    accuracy_pre = []

    # Loop through each fold
    for train_index, val_index in kf.split(X_train):
        # Split the data into training and testing sets for this fold
        # print(train_index, val_index)
        X_train_fold, X_test_fold = X_train[train_index], X_train[val_index]
        y_train_fold, y_test_fold = y_train[train_index], y_train[val_index]
        X_train_fold, y_train_fold = oversample(X_train_fold, y_train_fold)

        # X_train_fold, y_train_fold = preprocess(X_train_fold, y_train_fold)
        # X_test_fold, y_test_fold = preprocess(X_test_fold, y_test_fold)
        y_train_fold = np.array(y_train_fold).reshape(-1, 1)
        y_test_fold = np.array(y_test_fold).reshape(-1, 1)
        y_train_fold, y_test_fold = transform(y_train_fold, y_test_fold)

        # X_test_fold = tf.image.resize(X_test_fold, [64, 64]).numpy()

        train_dataset = tf.data.Dataset.from_tensor_slices((X_train_fold, y_train_fold))

        augmented_train_dataset = train_dataset.map(augment_image)
        augmented_train_dataset = augmented_train_dataset.shuffle(buffer_size=len(X_train_fold)).batch(batch_size)


        # print("fold", X_test_fold.shape, y_test_fold.shape)
        # print("fold", X_train_fold.shape, y_train_fold.shape)


        vgg = build_model()
        vgg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        early = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=1, mode='auto')
        cbks = [checkpoint, early]
        history = vgg.fit(augmented_train_dataset.repeat(),
                # class_weight=class_weight_dict,
                steps_per_epoch=391, #len(X_train_fold) // batch_size,
                epochs=180,
                callbacks = cbks,
                verbose = 1)

        loss, accuracy = vgg.evaluate(X_test_fold, y_test_fold, verbose=0)
        fold_loss.append(loss)
        fold_accuracy.append(accuracy)

        y_test = np.array(y_test).reshape(-1, 1)
        PREDICTED_CLASSES = vgg.predict(X_test, verbose=0)
        temp = sum(np.argmax(y_test, axis=1) == np.argmax(PREDICTED_CLASSES, axis=1))
        acc = temp / len(y_test)
        # print(acc)
        # Append scores to lists
        accuracy_pre.append(acc)



    average_accuracy = np.mean(accuracy_pre)


    print("prediction Accuracy:", average_accuracy)

    print(f'Average loss: {np.mean(fold_loss)}')
    print(f'Average accuracy: {np.mean(fold_accuracy)}')
    # vgg.save('model.h5')


    # vari=vgg.predict(X_test.reshape((X_test.shape[0],32,32,1)))
    # vari2=np.argmax(vari,axis=1)
    # print("starting evaluating")
    # # vgg.evaluate(X_test, y_test, verbose=0)
    # print("done dude")
    # accu = []
    # for i in range(100):
    #     model3 = build_model()
    #     model3.load_weights('mal.hdf5')
    #
    #     PREDICTED_CLASSES = model3.predict(X_test, verbose=0)
    #     # acc = validate_accuracy(y_test, PREDICTED_CLASSES)
    #     temp = sum(np.argmax(y_test, axis=1) == np.argmax(PREDICTED_CLASSES, axis=1))
    #     acc = temp / len(y_test)
    #     accu.append(acc)
    # accnew = np.mean(accu)
    # print("\nTest accuracy: %.1f%%" % (100.0 * accnew))