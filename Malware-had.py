import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
# from tensorflow import keras
from tensorflow.keras import regularizers
gpu_available = tf.config.list_physical_devices('GPU')

#Print TensorFlow version
print("TensorFlow version:", tf.__version__)

physical_devices = tf.config.list_physical_devices('GPU')
print(physical_devices)
# tf.config.experimental.set_memory_growth(physical_devices[0], True)


from tensorflow.keras import backend
# from keras.preprocessing.image import load_img
# from keras.preprocessing.image import img_to_array
# from keras.applications.vgg16 import preprocess_input
# from keras.applications.vgg16 import decode_predictions
from keras.applications.vgg16 import VGG16
from tensorflow.keras.models import Sequential
from keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D,Dropout,Activation,BatchNormalization
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.callbacks import LearningRateScheduler
# from keras.optimizers import Adam
from scipy.linalg import hadamard
from scipy.spatial import distance
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True  # Allow GPU memory growth
session = tf.compat.v1.Session(config=config)
tf.compat.v1.keras.backend.set_session(session)
from sklearn.metrics import classification_report
# print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
# print("Is CUDA available for TensorFlow:", tf.test.is_built_with_cuda())
# TF_GPU_ALLOCATOR= cuda_malloc_async
H = hadamard(32)
H[H == -1] = 0
H = H[1:26]
print(H)
# def scheduler(epoch):
#     if epoch < 5:
#         return 0.04
#     if epoch < 10:
#         return 0.015
#     return 0.005
dropout = 0.25
def hadamard_encode(x, H):
    encoded = np.zeros((len(x), 32))
    for idx, val in enumerate(x):
        # for item in val:
        #     curr = int(item)
        encoded[idx] = H[val]
    return encoded

def load_data(dataset, standardize=True):
    """
    :param dataset:
    :param standardize:
    :return:
    """

    features = dataset['arr'][:, 0]
    features = np.array([feature for feature in features])
    features = np.reshape(
        features, (features.shape[0], features.shape[1] * features.shape[2]))

    if standardize:
        features = StandardScaler().fit_transform(features)

    labels = dataset['arr'][:, 1]
    labels = np.array([label for label in labels])

    return features, labels

def build_model():
    model = Sequential()
    model.add((Conv2D(32, (3, 3), padding='valid', activation='relu', kernel_initializer='he_normal',
                       kernel_regularizer=regularizers.l2(0.001), input_shape=(32, 32, 1))))
    # model.add(GaussianNoise(0.5))
    # model.add(quantize_annotate_layer(Activation('relu')))

    model.add(BatchNormalization())

    model.add((Conv2D(32, (3, 3), padding='valid', activation='relu',
                        kernel_initializer='he_normal')))
    # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(64, (3, 3), padding='valid', activation='relu',
                      kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_normal')))
    # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(64, (3, 3), padding='valid', activation='relu',
                       kernel_initializer='he_normal')))
    # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(128, (3, 3), padding='valid', activation='relu',
                      kernel_regularizer=regularizers.l2(0.001), kernel_initializer='he_normal')))
    # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(128, (3, 3), padding='valid', activation='relu',
                       kernel_initializer='he_normal')))
    model.add(BatchNormalization())

    model.add((Conv2D(256, (3, 3), padding='valid', activation='relu',
                       kernel_initializer='he_normal')))
    model.add(BatchNormalization())

    model.add((Conv2D(256, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    # model.add(BatchNormalization())

    model.add(Flatten())
    model.add((Dense(1024, activation='relu', kernel_initializer='he_normal')))  # relu
    # model.add(Dropout(dropout))
    model.add((Dense(512, activation='relu', kernel_initializer='he_normal')))  # relu
    model.add((Dense(32, activation='sigmoid', kernel_initializer='he_normal')))  # softmax

    return model
# def build_model():
#     vgg = Sequential()
#     vgg.add(Conv2D(3, (3, 3), padding='same',input_shape=(32,32,1)))
#     vgg.add(Activation('relu'))
#
#     _vgg = VGG16(weights='imagenet', include_top=False)
# #or
# # _vgg = VGG19(weights='imagenet', include_top=False)
#
#     counter=0
#     for layer in _vgg.layers:
#         layer.trainable = False
#         counter+=1
#     print("VGG's ", counter , " layers are not added to the layer")
#     vgg.add(_vgg)
#     print("done")
#
#     vgg.add(Flatten())
#
#     vgg.add(Dense(128,activation='relu'))
#     vgg.add(Dense(32,activation="sigmoid"))
#     return vgg
# vgg.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
batch_size = 20
epochs  = 80

def new_accuracy(y_true, y_pred):
    y_pred_thre = backend.cast(backend.greater(backend.clip(y_pred, 0, 1), 0.5), dtype = 'float32')
    res = backend.mean(backend.cast(backend.equal(y_pred_thre, y_true), dtype = 'float32'), axis=1)
    accuracy = backend.mean(backend.cast(backend.equal(res, 1), dtype = 'float32'))
    return accuracy

def hamming_dis(y_pred, H):
    dis = distance.cdist(y_pred, H, 'euclidean')
    index = np.argmin(dis, axis=1)  # 按每行求出最小值的索引 euclidean
    true = []
    for i in index:
        true.append(H[i])
    # print(true[12:14])
    return true

def validate_accuracy(y_true, y_pred):
    # y_pred_thre = tf.cast(tf.greater(tf.clip_by_value(y_pred, 0, 1), 0.5), dtype='float32')
    # print((np.greater(np.clip(y_pred, 0, 1), 0.5).astype(float))[12:14])
    y_predm = tf.convert_to_tensor(hamming_dis(y_pred, H))
    res = tf.reduce_mean(tf.cast(tf.equal(y_predm, y_true), dtype = 'float32'), axis=1)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(res, 1), dtype = 'float32'))
    return accuracy

if __name__ == '__main__':
    data, labels = load_data(np.load('malimg.npz', allow_pickle=True))
    labels = labels.reshape((9339, 1))
    print("data.shape  = ", data.shape)
    print("labels.shape  = ", labels.shape)
    # Assuming train_labels is a numpy array of integer labels

    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.101)
    smote = SMOTE(random_state=42)
    X_resampled_train, y_resampled_train = smote.fit_resample(X_train, y_train)

    print("size", len(y_resampled_train))
    # print(y_test)
    # enc = OneHotEncoder()
    # enc.fit(y_train)
    # print(y_train.shape)
    # y_train = enc.transform(y_train).toarray()
    # y_test = enc.transform(y_test).toarray()
    # y_train_list = y_train.tolist()
    # print(y_train_list)
    # y_train_list = [label[0] for label in y_resampled_train]

    # class_labels = np.unique(y_train)
    # class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)

    # Convert class labels to integers (if they are not already)
    # class_labels_int = list(map(int, class_labels))

    # Create a dictionary mapping class labels (ints) to class weights
    # class_weight_dict = dict(zip(class_labels_int, class_weights))
    # print(class_weight_dict)
    y_resampled_train = hadamard_encode(y_resampled_train, H)
    y_test = hadamard_encode(y_test, H)

    # rehsaping traing
    print("X_train.shape before  = ", X_resampled_train.shape)
    X_resampled_train = X_resampled_train.reshape((len(X_resampled_train), 32, 32, 1))
    print("X_train.shape after  = ", X_resampled_train.shape)
    print("y_train.shape  = ", y_resampled_train.shape)

    # rehsaping testing
    print("X_test.shape before  = ", X_test.shape)
    X_test = X_test.reshape((len(X_test), 32, 32, 1))
    print("X_test.shape after  = ", X_test.shape)
    print("y_test.shape  = ", y_test.shape)
    X_resampled_train = X_resampled_train.astype('float32')
    X_test = X_test.astype('float32')
    # val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)
    # Compute class weights

    # Convert class weights to a dictionary to be passed to the model

    filepath = "malhad.hdf5"  ##lecif10 use for weights disturbance
    checkpoint = ModelCheckpoint(filepath, monitor='val_new_accuracy', verbose=1, save_best_only=True,
                                 mode='max')
    early = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')
    vgg = build_model()
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=0.01,
        decay_steps=10000,
        decay_rate=0.9)
    sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
    # opt = Adam(lr=0.1)
    # vgg.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

    vgg.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=sgd, metrics=[new_accuracy])
    # change_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)
    cbks = [checkpoint, early]

    datagen = ImageDataGenerator(horizontal_flip=True,
                                 width_shift_range=0.125, height_shift_range=0.125, fill_mode='constant', cval=0.)
    # datagen.fit(X_train)
    # val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)
    num_splits = 5

    # Initialize KFold cross-validation
    kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)

    # Initialize a list to store the accuracy scores
    accuracy_scores = []

    # Loop through each fold
    for train_index, test_index in kf.split(X_resampled_train):
        # Split the data into training and testing sets for this fold
        X_train_fold, X_test_fold = X_resampled_train[train_index], X_resampled_train[test_index]
        y_train_fold, y_test_fold = y_resampled_train[train_index], y_resampled_train[test_index]

        # Create and train your network model (assuming a model called 'model' is defined)
        vgg.fit(datagen.flow(X_train_fold, y_train_fold, batch_size=batch_size),
                # class_weight=class_weight_dict,
                steps_per_epoch=391,
                epochs=180,
                callbacks=cbks,
                validation_data=(X_test_fold, y_test_fold))

        # Evaluate the model on the testing set for this fold
        PREDICTED_CLASSES = vgg.predict(X_test_fold, verbose=0)
        # print(PREDICTED_CLASSES)
        acc = validate_accuracy(y_test_fold, PREDICTED_CLASSES)

        accuracy_scores.append(acc)
    average_accuracy = np.mean(accuracy_scores)
    print("Average Accuracy:", average_accuracy)

