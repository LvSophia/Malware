import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from tensorflow.keras import backend
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from keras.applications.vgg16 import VGG16
from keras.models import Sequential
from keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D,Dropout,Activation,BatchNormalization
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.callbacks import LearningRateScheduler
from keras.optimizers import Adam
from scipy.linalg import hadamard
from scipy.spatial import distance
from sklearn.utils import class_weight
H = hadamard(32)
H[H == -1] = 0
H = H[1:26]
# print(H)
# def scheduler(epoch):
#     if epoch < 5:
#         return 0.04
#     if epoch < 10:
#         return 0.015
#     return 0.005
dropout = 0.25
def hadamard_encode(x, H):
    encoded = np.zeros((len(x), 32))
    for idx, val in enumerate(x):
        for item in val:
            curr = int(item)
        encoded[idx] = H[curr]
    return encoded

def load_data(dataset, standardize=True):
    """
    :param dataset:
    :param standardize:
    :return:
    """

    features = dataset['arr'][:, 0]
    features = np.array([feature for feature in features])
    features = np.reshape(
        features, (features.shape[0], features.shape[1] * features.shape[2]))

    if standardize:
        features = StandardScaler().fit_transform(features)

    labels = dataset['arr'][:, 1]
    labels = np.array([label for label in labels])

    return features, labels

def build_model():
    model = Sequential()
    model.add((Conv2D(32, (3, 3), padding='valid', activation='relu', kernel_initializer='he_normal',
                      input_shape=(32, 32, 1))))
    # model.add(GaussianNoise(0.5))
    # model.add(quantize_annotate_layer(Activation('relu')))
    # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(32, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(64, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(64, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(128, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    # model.add(MaxPooling2D((2, 2), strides=(2, 2)))
    model.add(BatchNormalization())

    model.add((Conv2D(128, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    model.add(BatchNormalization())

    model.add(BatchNormalization())

    model.add((Conv2D(256, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    model.add(BatchNormalization())

    model.add((Conv2D(256, (3, 3), padding='valid', activation='relu',
                      kernel_initializer='he_normal')))
    model.add(BatchNormalization())

    model.add(Flatten())
    model.add((Dense(1024, activation='relu', kernel_initializer='he_normal')))  # relu
    # model.add(Dropout(dropout))
    model.add((Dense(512, activation='relu', kernel_initializer='he_normal')))  # relu
    model.add((Dense(32, activation='sigmoid', kernel_initializer='he_normal')))  # softmax

    return model
# def build_model():
#     vgg = Sequential()
#     vgg.add(Conv2D(3, (3, 3), padding='same',input_shape=(32,32,1)))
#     vgg.add(Activation('relu'))
#
#     _vgg = VGG16(weights='imagenet', include_top=False)
# #or
# # _vgg = VGG19(weights='imagenet', include_top=False)
#
#     counter=0
#     for layer in _vgg.layers:
#         layer.trainable = False
#         counter+=1
#     print("VGG's ", counter , " layers are not added to the layer")
#     vgg.add(_vgg)
#     print("done")
#
#     vgg.add(Flatten())
#
#     vgg.add(Dense(128,activation='relu'))
#     vgg.add(Dense(32,activation="sigmoid"))
#     return vgg
# vgg.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
batch_size = 20
epochs  = 80

def new_accuracy(y_true, y_pred):
    y_pred_thre = backend.cast(backend.greater(backend.clip(y_pred, 0, 1), 0.5), dtype = 'float32')
    res = backend.mean(backend.cast(backend.equal(y_pred_thre, y_true), dtype = 'float32'), axis=1)
    accuracy = backend.mean(backend.cast(backend.equal(res, 1), dtype = 'float32'))
    return accuracy

def hamming_dis(y_pred, H):
    dis = distance.cdist(y_pred, H, 'euclidean')
    index = np.argmin(dis, axis=1)  # 按每行求出最小值的索引 euclidean
    true = []
    for i in index:
        true.append(H[i])
    # print(true[12:14])
    return true

def validate_accuracy(y_true, y_pred):
    # y_pred_thre = tf.cast(tf.greater(tf.clip_by_value(y_pred, 0, 1), 0.5), dtype='float32')
    # print((np.greater(np.clip(y_pred, 0, 1), 0.5).astype(float))[12:14])
    y_predm = tf.convert_to_tensor(hamming_dis(y_pred, H))
    res = tf.reduce_mean(tf.cast(tf.equal(y_predm, y_true), dtype = 'float32'), axis=1)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(res, 1), dtype = 'float32'))
    return accuracy

if __name__ == '__main__':
    data, labels = load_data(np.load('malimg.npz', allow_pickle=True))
    labels = labels.reshape((9339, 1))
    print("data.shape  = ", data.shape)
    print("labels.shape  = ", labels.shape)
    # Assuming train_labels is a numpy array of integer labels

    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.101)
    # print(y_test)
    # enc = OneHotEncoder()
    # enc.fit(y_train)
    # print(y_train.shape)
    # y_train = enc.transform(y_train).toarray()
    # y_test = enc.transform(y_test).toarray()
    y_train_list = y_train.tolist()
    # print(y_train_list)
    y_train_list = [label[0] for label in y_train_list]
    # print(y_train_list)
    class_labels = np.unique(y_train_list)
    class_weights = class_weight.compute_class_weight('balanced', class_labels, y_train_list)

    # Convert class labels to integers (if they are not already)
    class_labels_int = list(map(int, class_labels))

    # Create a dictionary mapping class labels (ints) to class weights
    class_weight_dict = dict(zip(class_labels_int, class_weights))
    print(class_weight_dict)
    y_train = hadamard_encode(y_train, H)
    y_test = hadamard_encode(y_test, H)

    # rehsaping traing
    print("X_train.shape before  = ", X_train.shape)
    X_train = X_train.reshape((len(X_train), 32, 32, 1))
    print("X_train.shape after  = ", X_train.shape)
    print("y_train.shape  = ", y_train.shape)

    # rehsaping testing
    print("X_test.shape before  = ", X_test.shape)
    X_test = X_test.reshape((len(X_test ), 32, 32, 1))
    print("X_test.shape after  = ", X_test.shape)
    print("y_test.shape  = ", y_test.shape)
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    # val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)
    # Compute class weights


    # Convert class weights to a dictionary to be passed to the model

    filepath = "malhad.hdf5"  ##lecif10 use for weights disturbance
    checkpoint = ModelCheckpoint(filepath, monitor='val_new_accuracy', verbose=1, save_best_only=True,
                                 mode='max')
    early = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')
    vgg = build_model()
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=0.1,
        decay_steps=10000,
        decay_rate=0.9)
    sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
    # opt = Adam(lr=0.1)
    # vgg.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

    vgg.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=sgd, metrics=[new_accuracy])
    # change_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)
    cbks = [checkpoint, early]

    datagen = ImageDataGenerator(horizontal_flip=True,
                                 width_shift_range=0.125, height_shift_range=0.125, fill_mode='constant', cval=0.)
    # datagen.fit(X_train)
    val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)
    vgg.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                      class_weight=class_weight_dict,
                      steps_per_epoch=391,
                      epochs=180,
                      callbacks=cbks,
                      validation_data=(X_test, y_test))
    # vgg.save('model.h5')


    # vari=vgg.predict(X_test)
    # acc = validate_accuracy(y_test, vari)
    # # vari2=np.argmax(vari,axis=1)
    # print("starting evaluating")
    # # vgg.evaluate(X_test, y_test, verbose=0)
    # print("done dude")

    accu = []
    for i in range(100):
        model3 = build_model()
        model3.load_weights("malhad.hdf5")    #hamin for final
        weights1 = model3.get_weights()
        PREDICTED_CLASSES = model3.predict(X_test, verbose=0)
        print()
        acc = validate_accuracy(y_test, PREDICTED_CLASSES)
        accu.append(acc)
    accnew = np.mean(accu)
    print("\nTest accuracy: %.1f%%" % (100.0 * accnew))